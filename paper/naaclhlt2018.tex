%
% File naaclhlt2018.tex
%
%% Based on the style files for NAACL-HLT 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{naaclhlt2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Experiments with Universal CEFR Classification}
%Working title

%\author{Sowmya Vajjala\\
%  {\tt email@domain} \\\And
%  Taraka Rama \\
%  {\tt email@domain} \\}

\date{}

%When you first create your submission on softconf, please fill in your submitted paper ID where {\small\verb|***|} appears in the {\small\verb|\def\aclpaperid{***}|} definition at the top.

\begin{document}
\maketitle
\begin{abstract}
In this paper, we explore the idea of performing multi-language automated essay scoring using Universal Depdencies framework and the CEFR scale of language proficiency assessment. We conducted experiments in mono-lingual, cross-lingual, and multi-lingual classification using a range of language specific, and common features. We performed our experiments using the MERLIN dataset which supports three languages-German, Czech and Italian. Our results show that XX XX XX
\end{abstract} %note: Limit -200 words

\section{Introduction}
Automated Essay Scoring (AES) refers to the task of automatically grading student essays written in response to some prompt using a pre-defined scale of measurement. The goal of AES is to mimic a human grader interms of evaluating a student essay. AES is commonly used as a method of predicting a student's language proficiency in high stakes assessments such as GRE and TOEFL. The scale of measurement is not uniform and varies from country to country, and test to test. Common European Framework of Reference for languages (CEFR) is one such scale guideline used to describe the language abilities of learners across Europe at different levels of proficiency in a foreign language. 

While there is a lot of research into English-AES in general, there is not much work in AES using CEFR categorization. Further, to our knowledge, all the work in AES described approaches that worked on one language. The idea of performing cross-lingual AES was not explored so far. On one hand, using an essay scoring model developed for one language with another seems unacceptable. On the other hand, the CEFR guidelines are indeed universal, which would lead us to hypothesize there could be a common idea of "proficiency" encoded in AES systems that can potentially work across languages. Further, having such a working approach could also be beneficial for quick prototyping of CEFR grading systems for languages without ready made training data, and for comparing human and machine grading. 

In this paper, we explore this hypothesis by exploring CEFR-classification for three languages-German, Italian and Czech. Apart from constructing individual CEFR classification models using some common text classification and AES specific features, we also looked into cross-lingual classification (i.e., training on one language, and testing on another) and multi-lingual classification (i.e., building one large classification model with all the three languages as one dataset). To do the last two, we need a common feature representation across languages. Word/Character n-grams and embeddings, which are among the most commonly used features in text classification will not work in those cases. We used features derived from Universal Dependencies based parsers, which achieve a common representation across languages at the POS and dependency relation level. 

In short, we study the possibility of a universal CEFR using universal Dependencies framework. Thus, this paper contributes and extends the existing AES research in NLP community by proposing new, generalizable feature representations that can work with multiple languages. To summarize, the contributions of this paper are as follows: 
\begin{enumerate}
\item 
\item 
\item 
\end{enumerate}

The rest of this paper is organized as follows: Section~\ref{sec:Approach} describes our data, features, methodology and evaluation setup. Section~\ref{sec:results} discuss in detail our experiments and results. Section~\ref{sec:related} describes some of the related work and establishes the current results in that context. Section~\ref{sec:conclusion} concludes the paper with pointers to future work.

\section{Approach} %Some methods section
\label{sec:approach}

\subsection{Dataset}

%mention-files removed. 

\subsection{Features}

\subsection{Classification and Evaluation}
%mention general approach and evaluation method?

\section{Experiments and Results}
\label{sec:results}

\subsection{Monolingual classification}


\subsection{Cross-lingual classification}


\subsection{Multi-lingual classification}


\begin{table}[t!]
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
captions & 10 pt & \\
abstract text & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

\section{Related Work}
\label{sec:related}

\section{Conclusion}
\label{sec:conclusion}


\bibliography{naaclhlt2018}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}
The code for our experiments, and UDPipe output files we used to run our experiments are uploaded as a tar file with this submission.


\end{document}

\section*{Acknowledgments}

The acknowledgments should go immediately before the references.  Do
not number the acknowledgments section. Do not include this section
when submitting your paper for review.


\section{Credits}
Outline: 
What is the point of cross-lingual scoring?
Is it even possible? 
How do we set about testing the idea?

Features: 
Char-embeddings, Word embeddings (per Lang separately - baseline)
Char-embeddings (for all languages put together) - Zero shot scoring kind of thing.
[+Logistic Reg, SVM, LSTM]

POS n-grams 
Dep triad n-grams
[+logistic reg, SVM, other algorithms]

Summary: table of the form:
train-lang, test-lang, performance

can do both classification and regression

Possible plans for extension: Adding Estonian/Norwegin etc... which have learner data, which have UD parsers
adding features related to spelling and grammar errors
feature ensembles
etc
