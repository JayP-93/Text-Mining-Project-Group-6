Doing: take all data as if it belongs to one large dataset, and do classification
Mega classification for:  domain  features
Distribution of labels: 
Counter({'B1': 890, 'A2': 875, 'B2': 374, 'A1': 86, 'C1': 42})
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_split=1e-07,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
[ 0.54585153  0.5720524   0.5745614   0.64035088  0.64317181  0.67256637
  0.69777778  0.63555556  0.56888889  0.62666667]
0.617744327757
[[ 23  63   0   0   0]
 [ 19 688 162   6   0]
 [  1 218 565 101   5]
 [  0  10 182 179   3]
 [  0   0  16  26   0]]
0.448644728091
LinearSVC(C=1.0, class_weight='balanced', dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0)
[ 0.49781659  0.38427948  0.61842105  0.42105263  0.3876652   0.65486726
  0.39555556  0.28        0.2         0.52      ]
0.435965776451
[[ 24  59   3   0   0]
 [174 541 102  50   8]
 [131 343 167 244   5]
 [ 29  79  95 169   2]
 [  3   5  11  22   1]]
0.273525581839
LogisticRegression(C=1.0, class_weight='balanced', dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)
[ 0.54585153  0.51965066  0.54385965  0.57894737  0.64757709  0.65486726
  0.67111111  0.63111111  0.57333333  0.54222222]
0.590853132788
[[ 37  49   0   0   0]
 [ 48 696 120   5   6]
 [  1 283 404 196   6]
 [  0  10 145 199  20]
 [  0   0   4  35   3]]
0.448945500234
